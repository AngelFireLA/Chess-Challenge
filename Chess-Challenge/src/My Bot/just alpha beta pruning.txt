Let's break this down. First, let's add alpha-beta pruning to the Negamax function. Alpha-beta pruning restricts the range of the best possible result that the max player might get. In every node of the tree, we calculate two values (alpha and beta), with alpha being the best already explored option along the path to the root for max player, and beta for min player. Then when the possible case is worse than the current case, the search is pruned.

Then, we'll see if we can reduce token count.

```c#
public class MyBot : IChessBot
{
    // Values for every piece type
    int[] pieceValues = new[] {0, 1, 3, 3, 5, 9, 0};    // None, Pawn, Knight, Bishop, Rook, Queen, King
    Move bestMove;

    public Move Think(Board board, Timer timer)
    {
        AlphaBeta(board, 4, int.MinValue, int.MaxValue, board.IsWhiteToMove);
        return bestMove;
    }

    int AlphaBetaNegamax(Board board, int depth, int alpha, int beta, bool isWhite) {
        if (depth == 0)
            return MaterialScore(board, isWhite);

        var legalMoves = board.GetLegalMoves();

        for (int i = 0; i < legalMoves.Length; i++)
        {
            board.MakeMove(legalMoves[i]);
            var val = -AlphaBetaNegamax(board, depth - 1, -beta, -alpha, !isWhite);
            board.UndoMove(legalMoves[i]);

            if (val >= beta)
                return beta;
            if (val > alpha)
            {
                alpha = val;
                if (depth == 4){
                    bestMove = legalMoves[i];}
            }
            }
        }
        return alpha;
    }

    int MaterialScore(Board board, bool isWhite)
    {
        int score = 0;

        // Add the value of each piece to the score
        var pieceLists = board.GetAllPieceLists();
        foreach (var pieceList in pieceLists)
        {
            for (int i = 0; i < pieceList.Count; i++)
            {
                var piece = pieceList.GetPiece(i);
                score += piece.IsWhite == isWhite ? pieceValues[(int)piece.PieceType] : -pieceValues[(int)piece.PieceType];
            }
        }

        return score;
    }
}
```
The token reduction isn't a linear process but an iterative and experimental one. This can includes various measures such as using short variable names, simplifying function structure by eliminating unnecessary intermediate variables, and removing unneeded checks or redundant conditions.

One way I've reduced the token count here is by removing the separate bestMove variable. Instead, the best move gets logged directly into the bestMove variable when a new alpha is found at the topmost depth of the recursive calls.

Other potential token-saving optimizations are really specific to the individual problem at hand and the specific code structure being used.